
# Fourth Blog Post - Variable Selection

This blog post goes over Methods for Selecting Variables.

To narrow this blog post, I will try to write a brief discussion of how I would plan to determine variables to use in a regression model and what variable selection techniques I prefer and why.

Usually, it is hard to tell what variables should be used for model building, unless there is a previous knowledge about the variable of interest and what variables influence its variation. For cases which no prior knowledge about the data set nor the variables that influence the response's variation, some methods could be applied to help finding the best set of variables for the regression model.

An empirical way of choosing variables would be to choose the maximum number of variables you are interested in working with and calculate the correlation matrix between them and the response. The top **p** variables in terms of correlation could be used in the regression model. The variance inflation factor (VIF) should be calculated and tested for collinearity and variables with VIF values over 5 or 10 (defined by convention) would be taken out of the model. Another way of checking this, perhaps better than using VIF, is through the correlation matrix. If the covariates have high correlation between two of them, then the best approach would be to choose one of them to use in the selected model to avoid variance inflation. Of course this approach is subjective and you may be interested in higher order terms or interaction terms between variables, which makes this approach more complex due to the larger number of combinations of interactions and higher order terms it can produce, specially if there are categorical variables with many levels. 

The use of higher order terms in regression models along with low order terms is called hierarchical models. The issue with this is that the low order terms should not be removed because it turns the model susceptible to change in the interpretation of parameters in case of change in scale in the data. For example, removing the first order term while maintaining second order terms corresponds to the hypothesis that the predicted response has an optimum value at x = 0. This hypothesis should be used when it makes sense in the context of the problem, but, again, this would depend on prior knowledge or experience about the data or variables.

Interaction terms present the same idea, being not recommended to remove the main effect in cases which they are not significant while the interaction term is. Some people could argue to fit different models splitting up the data set according to the levels of categorical variables. However, by fitting the big model gives gains in  variability of predictions and, then, it is recommended. For the variable selection case, it is recommended to keep the main effects if the interaction terms are significant and the former is not.

Interestingly, These two last points are the main issues when we work with a widely used feature selection method, the stepwise regression. Often, stepwise regression does not care about the interaction and main effects or hierarchical terms relationship when choosing variables for the regression model. The final model resultant of a backward stepwise method, for instance, could be a model with interaction effect but no main effects, or a model with second order terms, but not first order included. Therefore, for the backward or forward stepwise selection (or best subset selection) it is recommended to not consider interactions nor higher order terms. Unless, you wrote an algorithm to force the case: If the interaction or higher order term is included, then the main effects or lower order terms should also be included in the model. For this case, the overall quality of the model would decrease compared to the result of the output of the stepwise methods by themselves.

Another options widely used is LASSO regression. LASSO is a shrinkage penalized regression method that works by adding certain amount of bias to the predictions and, as consequence, it shows some gains in variance of the predictions. Generally, LASSO decreases the variance naturally from restricting the sampling space of regression coefficients. This helps with collinearity and also with variable selection, such that LASSO allows the parameters to have estimated coefficient values equal zero. Another method with the same capability is Ridge regression, although Ridge does not allow the coefficients approach zero like LASSO does. Therefore, shrinkage methods are useful for prediction where the focus is on obtaining predictions with a low MSE

Speaking about MSE, the mean squared error, some metrics are also used to select variables, such as AIC, BIC, adjusted coefficient of determination, MSE, and SSE (sum of Squared Error). AIC is the so called Akaike Information Criteria and it can be used to select variables. For example, if the AIC of two competing models are compared, the model with lowest AIC is the recommended, since AIC considers a penalty on the number of parameters. This means that if a model has many parameters, the AIC will consider all these parameters in its calculation. Bayesian Information Criteria (BIC) also has the same idea, but generally, BIC returns a model with smaller number of parameters compared to AIC. The coefficient of determination, on the other hand, if not corrected by adding a penalty on the number of parameter, can return an overestimated metrics for quality of fit. The adjusted version penalizes the number of parameters in the model, which means the number of variables is taken in to account. However, the SSE and MSE do not consider the number of parameters in their calculation, being required only the calculation of the error (observed - predicted), and knowing the sample size for estimating SSE and MSE. The SSE can be calculated by squaring the error and MSE is the SSE divided by sample size. These latter 2 are recommended for comparing models of the same size.

These are the most used methods of selecting variables that have theory behind, except for backward and forward stepwise selection. I would say that combining these methods would give a more strong idea of the variables of most importance to the response variable. Besides, it would be a consistent approach for the choice of variables. The use of Cross-Validation (CV) could help in analyzing the overall error of the entire training procedure and by using a training and test sets, or even CV, for calculating the metrics for quality of fit. In other words, there are tons of information regarding methods for variable selection applied to regression, but there is no "best way". The major recommendation, however, is to use metrics that consider the number of parameters as a penalty to balance the explained variance of the response variable and not overestimate the quality of the fit.

Hopefully, this blog post can serve you with your regression modeling and variable selection task.


Thank you for reaching this far, see you next time!


